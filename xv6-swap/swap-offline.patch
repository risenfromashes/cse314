diff --git a/kernel/proc.c b/kernel/proc.c
index 1c50742..f147a7a 100644
--- a/kernel/proc.c
+++ b/kernel/proc.c
@@ -290,14 +290,15 @@ fork(void)
 
   // release lock so that swapin and swapout can be called
   release(&np->lock);
+
   // Copy user memory from parent to child.
   if(uvmcopy(p->pagetable, np->pagetable, p->sz) < 0){
     freeproc(np);
     return -1;
   }
 
+  // reacquire locks
   acquire(&np->lock);
-  // re-enable interrupts
 
   np->sz = p->sz;
 
@@ -424,6 +425,7 @@ wait(uint64 addr)
           freeproc(pp);
           release(&pp->lock);
           release(&wait_lock);
+          // locks need to be released to free pagetable, as it may call swapin/out
           proc_freepagetable(pt, sz);
           return pid;
         }
diff --git a/kernel/vm.c b/kernel/vm.c
index d215db3..99c8044 100644
--- a/kernel/vm.c
+++ b/kernel/vm.c
@@ -211,6 +211,7 @@ uvmunmap(pagetable_t pagetable, uint64 va, uint64 npages, int do_free)
     if(do_free){
       int flags = PTE_FLAGS(*pte);
       if (flags & PTE_S) {
+        // page swapped out, so free
         swapfree((struct swap *)PTE2SW(*pte));
         __sync_fetch_and_add(&pagecount, -1);
       } else {
@@ -250,14 +251,13 @@ uvmfirst(pagetable_t pagetable, uchar *src, uint sz)
     panic("uvmfirst: more than a page");
   pte_t *pte = walk(pagetable, 0, 1);
   int flags = PTE_W|PTE_R|PTE_X|PTE_U;
-
+  // allocate mempage for first user process
   if((p = allocmempage(pagetable, pte, flags, NULL)) == 0) {
     panic("uvmfirst: couldn't allocate mempage");
   }
 
   char *mem = p->pa;
 
-  memset(mem, 0, PGSIZE);
   memmove(mem, src, sz);
 }
 
@@ -280,6 +280,7 @@ uvmalloc(pagetable_t pagetable, uint64 oldsz, uint64 newsz, int xperm)
     }
     int flags = PTE_R|PTE_U|xperm;
     struct mempage *p;
+    // allocate a mem page, allocmempage takes care of everything
     if ((p = allocmempage(pagetable, pte, flags, NULL)) == 0) {
       uvmdealloc(pagetable, a, oldsz);
       return 0;
@@ -370,18 +371,22 @@ uvmcopy(pagetable_t old, pagetable_t new, uint64 sz)
     }
 
     if (flags & PTE_S) {
-      // increase ref count of swap
+      // page already swapped out
+      // just increase ref count of swap
       struct swap *s = (struct swap *)PTE2SW(*pte);
       *new_pte = *pte;
       swapref(s);
       __sync_fetch_and_add(&pagecount, 1);
     } else {
       struct mempage *p;
+      // try to allocate a mempage
       if((p = allocmempage(new, new_pte, flags, NULL)) == 0)
         goto err;
-
+      // copy atomically from pte if pte still holds pa, that is mempage associated with pte not yet swapped out
       if (mempagesafecopy(pte, pa, p->pa) != 0) {
+        // if copy fails, parent page already swapped out
         freemempagenode(p);
+        // just increase ref count
         struct swap *s = (struct swap *)PTE2SW(*pte);
         *new_pte = *pte;
         swapref(s);
@@ -417,6 +422,7 @@ copyout(pagetable_t pagetable, uint64 dstva, char *src, uint64 len)
 {
   uint64 n, va0, pa0;
 
+  // ensure mempage was loaded
   if (uvmloadpage(pagetable, dstva) < 0) {
     return -1;
   }
@@ -446,6 +452,7 @@ copyin(pagetable_t pagetable, char *dst, uint64 srcva, uint64 len)
 {
   uint64 n, va0, pa0;
 
+  // ensure mempage was loaded
   if (uvmloadpage(pagetable, srcva) < 0) {
     return -1;
   }
@@ -477,6 +484,7 @@ copyinstr(pagetable_t pagetable, char *dst, uint64 srcva, uint64 max)
   uint64 n, va0, pa0;
   int got_null = 0;
 
+  // ensure mempage was loaded
   if (uvmloadpage(pagetable, srcva) < 0) {
     return -1;
   }
